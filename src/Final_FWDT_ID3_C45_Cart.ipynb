{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bản sao của Final_FWDT_ID3_C45_Cart.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNaZr9u/Ty4VaNJTi6GIIP/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"t7K6GSHHLf2g","executionInfo":{"status":"ok","timestamp":1625769716281,"user_tz":-420,"elapsed":772,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}}},"source":["import pandas as pd\n","import numpy as np\n","import numpy.linalg as la\n","import random\n","from array import array\n","from collections import Counter\n","import operator\n","import time\n","\n","from random import randrange  \n","  \n","from sklearn.datasets import make_blobs  \n","from sklearn.preprocessing import normalize \n","\n","from math import log\n","from collections import Counter\n","from random import shuffle\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n","\n","\n","def calcShannonEnt(dataSet):  # Calculate the entropy of the data\n","    numEntries=len(dataSet)  # len of data\n","    labelCounts={}\n","    for featVec in dataSet:\n","        currentLabel=featVec[-1] # The last word (category) of each row of data\n","        if currentLabel not in labelCounts.keys():\n","            labelCounts[currentLabel]=0\n","        labelCounts[currentLabel]+=1  # Count number of classes there are and the number of each class\n","    shannonEnt=0\n","    for key in labelCounts:\n","        prob=float(labelCounts[key])/numEntries # Calculate the entropy of a single class\n","        shannonEnt-=prob*log(prob,2) # Accumulate the entropy of each class\n","    return shannonEnt\n","\n","def splitDataSet(dataSet,axis,value): # Split data by a certain feature (best feature)\n","    retDataSet=[]\n","    for featVec in dataSet:\n","        if featVec[axis]==value:\n","            reducedFeatVec =featVec[:axis]\n","            reducedFeatVec.extend(featVec[axis+1:])\n","            retDataSet.append(reducedFeatVec)\n","    return retDataSet\n","\n","def chooseBestFeatureToSplit(dataSet):  # Choose the best classification features\n","    numFeatures = len(dataSet[0])-1\n","    baseEntropy = calcShannonEnt(dataSet)  # Original entropy\n","    bestInfoGain = 0\n","    bestFeature = -1\n","    for i in range(numFeatures):\n","        featList = [example[i] for example in dataSet]\n","        uniqueVals = set(featList)\n","        newEntropy = 0\n","        for value in uniqueVals:\n","            subDataSet = splitDataSet(dataSet,i,value)\n","            prob =len(subDataSet)/float(len(dataSet))\n","            newEntropy +=prob*calcShannonEnt(subDataSet)  # Entropy after classification\n","        infoGain = baseEntropy - newEntropy  # Difference between the original entropy and the entropy classified by feature\n","        \n","        if (infoGain>bestInfoGain):   # If after dividing by a certain feature, the entropy value is reduced the most, then the secondary feature is the optimal classification feature\n","            bestInfoGain=infoGain\n","            bestFeature = i\n","    return bestFeature\n","\n","def cal_feature_weight22(data):\n","  #X,Y=convert_data2(data)\n","  df = pd.DataFrame(data)\n","\n","  #cal_feature_weight(data)\n","\n","  label=df.iloc[:,-1:]\n","\n","  df.drop(columns=df.columns[-1], axis=1,inplace=True)\n","  #convert data after delete label column to numpy array\n","  X=df.to_numpy()\n","\n","  #convert the label column to a row\n","  label=label.transpose()\n","\n","  #convert label to a matrix\n","  Y=np.concatenate((label.to_numpy()))\n","  n_sample,n_feature=np.shape(X)\n","\n","  my_label_vector = Y\n","  my_input_matrix=X\n","\n","  r = relief.Relief(n_features=n_feature)\n","\n","  my_transformed_matrix = r.fit_transform(my_input_matrix,my_label_vector)\n","\n","  weight=r.w_\n","\n","  #return np.delete(weight, -1)\n","\n","  return weight\n","\n","def choose_the_feat22(data):\n","  idx=np.argmax(cal_feature_weight22(data))\n","  return idx\n","\n","def createSubtable(dataTable, index, value):\n","    '''\n","    returns sub-dataTable based on the given dataTable, index of the attribute and its value.\n","    '''\n","\n","    subDataTable = []\n","    for row in dataTable:\n","        if row[index] == value:\n","            chopped_row = row[:index]  # setting the values that are BEFORE the index\n","            chopped_row.extend(row[index+1:])  # setting the value that are AFTER the index\n","            subDataTable.append(chopped_row) # adding both of them, eventually returning reduced dataTable.\n","    return subDataTable\n","    \n","def majority_voting(decision):\n","    '''\n","    returns the most popular element from the list given\n","    '''\n","\n","    return Counter(decision).most_common()[0][1]\n","\n","\n","def C45_chooseBestFeatureToSplit(dataset):\n","    numFeatures=len(dataset[0])-1\n","    baseEnt=calcShannonEnt(dataset)\n","    bestInfoGain_ratio=0.0\n","    bestFeature=-1\n","    for i in range(numFeatures): #Traverse all features\n","        featList=[example[i]for example in dataset]  \n","        uniqueVals=set(featList) #The feature list is created as a set collection, and the elements cannot be repeated. Create a unique category label list\n","        newEnt=0.0\n","        IV=0.0\n","        for value in uniqueVals:     #Calculate the information entropy of each division method\n","            subdataset=splitDataSet(dataset,i,value)\n","            p=len(subdataset)/float(len(dataset))\n","            newEnt+=p*calcShannonEnt(subdataset)\n","            IV=IV-p*log(p,2)\n","        infoGain=baseEnt-newEnt\n","        if (IV == 0): \n","            continue\n","        infoGain_ratio = infoGain / IV                    \n","        \n","        if (infoGain_ratio >bestInfoGain_ratio):          #Select the largest gain ratio\n","            bestInfoGain_ratio = infoGain_ratio\n","            bestFeature = i                              #Select the feature corresponding to the largest gain ratio\n","    return bestFeature\n","\n","def CART_chooseBestFeatureToSplit(dataset):\n","\n","    numFeatures = len(dataset[0]) - 1\n","    bestGini = 999999.0\n","    bestFeature = -1\n","    for i in range(numFeatures):\n","        featList = [example[i] for example in dataset]\n","        uniqueVals = set(featList)\n","        gini = 0.0\n","        for value in uniqueVals:\n","            subdataset=splitDataSet(dataset,i,value)\n","            p=len(subdataset)/float(len(dataset))\n","            subp = len(splitDataSet(subdataset, -1, '0')) / float(len(subdataset))\n","        gini += p * (1.0 - pow(subp, 2) - pow(1 - subp, 2))\n","        \n","        if (gini < bestGini):\n","            bestGini = gini\n","            bestFeature = i\n","    return bestFeature\n","\n","def majorityCnt(classList):    # Sort by the number of categories after classification, for example: the final classification is 2 male and 1 female, then it is judged as male;\n","    classCount={}\n","    for vote in classList:\n","        if vote not in classCount.keys():\n","            classCount[vote]=0\n","        classCount[vote]+=1\n","    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n","    return sortedClassCount[0][0]\n","\n","def ID3_createTree(dataSet,labels):\n","    classList=[example[-1] for example in dataSet]  \n","    if classList.count(classList[0])==len(classList):\n","        return classList[0]\n","    if len(dataSet[0])==1:\n","        return majorityCnt(classList)\n","    bestFeat=chooseBestFeatureToSplit(dataSet) \n","    bestFeatLabel=labels[bestFeat]\n","   \n","    myTree={bestFeatLabel:{}} #create the dict\n","    del(labels[bestFeat])\n","    featValues=[example[bestFeat] for example in dataSet]\n","    uniqueVals=set(featValues)\n","    for value in uniqueVals:\n","        subLabels=labels[:]\n","        myTree[bestFeatLabel][value]=ID3_createTree(splitDataSet(dataSet,bestFeat,value),subLabels)\n","    return myTree\n","\n","def C45_createTree(dataset,labels):\n","    classList=[example[-1] for example in dataset]\n","    if classList.count(classList[0]) == len(classList):\n","        # The categories are exactly the same, stop dividing\n","        return classList[0]\n","    if len(dataset[0]) == 1:\n","        # After traversing all features, return the most frequent occurrence\n","        return majorityCnt(classList)\n","    bestFeat = C45_chooseBestFeatureToSplit(dataset)\n","    bestFeatLabel = labels[bestFeat]\n","    \n","    C45Tree = {bestFeatLabel:{}}\n","    del(labels[bestFeat])\n","    # Get the list including all the attribute values of the node\n","    featValues = [example[bestFeat] for example in dataset]\n","    uniqueVals = set(featValues)\n","    for value in uniqueVals:\n","        subLabels = labels[:]\n","        C45Tree[bestFeatLabel][value] = C45_createTree(splitDataSet(dataset, bestFeat, value), subLabels)\n","    return C45Tree\n","\n","def CART_createTree(dataset,labels):\n","    classList=[example[-1] for example in dataset]\n","    if classList.count(classList[0]) == len(classList):\n","        # The categories are exactly the same, stop dividing\n","        return classList[0]\n","    if len(dataset[0]) == 1:\n","        # After traversing all features, return the most frequent occurrence\n","        return majorityCnt(classList)\n","    bestFeat = CART_chooseBestFeatureToSplit(dataset)\n","    \n","    bestFeatLabel = labels[bestFeat]\n","    \n","    CARTTree = {bestFeatLabel:{}}\n","    del(labels[bestFeat])\n","    # Get the list including all the attribute values of the node\n","    featValues = [example[bestFeat] for example in dataset]\n","    uniqueVals = set(featValues)\n","    for value in uniqueVals:\n","        subLabels = labels[:]\n","        CARTTree[bestFeatLabel][value] = CART_createTree(splitDataSet(dataset, bestFeat, value), subLabels)\n","    return CARTTree\n","\n","def FWDT_createTree(dataTable, labels):\n","    '''\n","    Generates tree based on the dataTable given. dataTable shrinks over time which allows recursive algorithm.\n","    '''\n","    #X,Y=convert_data(dataTable)\n","\n","    decision = [row[-1] for row in dataTable]\n","\n","    '''\n","    dataCounter = Counter(decision) #to find the most common class\n","    majorityClass = dataCounter.most_common(1)[0][0] #set the value to majorityClass\n","\n","    dataCounter = Counter(Y) #to find the most common class\n","    majorityClass = dataCounter.most_common(1)[0][0] #set the value to majorityClass\n","\n","    if Y.count(Y[0]) == len(Y):\n","        return Y[0]              # return when all of the decision in the dataTable is same\n","\n","    if len(X[0]) == 1:\n","        return majority_voting(Y)    # do the majority voting if there is only one remaining feature in dataTable\n","    '''\n","\n","    dataCounter = Counter(decision) #to find the most common class\n","    majorityClass = dataCounter.most_common(1)[0][0] #set the value to majorityClass\n","\n","    if decision.count(decision[0]) == len(decision):\n","        return decision[0]              # return when all of the decision in the dataTable is same\n","\n","    if len(dataTable[0]) == 1:\n","        return majority_voting(decision)    # do the majority voting if there is only one remaining feature in dataTable\n","\n","    #root_attribute = NodeSelection(dataTable)\n","    root_attribute = choose_the_feat22(dataTable)\n","\n","    root_attributeLabel = labels[root_attribute]\n","    tree = {root_attributeLabel: {}}\n","\n","    del (labels[root_attribute])  # reducing the dataTable so the recursion will work\n","    attributeValuesAll = [row[root_attribute] for row in dataTable]\n","    attribute_values = set(attributeValuesAll)  # finding the unique values\n","\n","    #attribute_values.add(None)\n","    for value in attribute_values:\n","        #if value == None:\n","           # tree[root_attributeLabel][value] = majorityClass #set the None branch to majority class\n","      sub_labels = labels[:]\n","      tree[root_attributeLabel][value] = FWDT_createTree(createSubtable(dataTable, root_attribute, value), sub_labels)\n","    \n","    return tree"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tY4YzrTqI6gD","executionInfo":{"status":"ok","timestamp":1625769719714,"user_tz":-420,"elapsed":3434,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}},"outputId":"cae60ea5-7f57-4404-c5c2-2d4722ed1506"},"source":["pip install sklearn_relief"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn_relief in /usr/local/lib/python3.7/dist-packages (1.0.0b2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_relief) (1.19.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_relief) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from sklearn_relief) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->sklearn_relief) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TmZPdgExI7aU","executionInfo":{"status":"ok","timestamp":1625769719715,"user_tz":-420,"elapsed":17,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}}},"source":["import sklearn_relief as relief\n","#from ReliefF import ReliefF"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWX7YON2S570","executionInfo":{"status":"ok","timestamp":1625769719715,"user_tz":-420,"elapsed":17,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}}},"source":["def convert_data_all(data,features):\n","  new_data=pd.DataFrame()\n","  for feature in features:\n","    col=data[feature]\n","    if (str(list(col)[0]).split(\".\")[0]).isdigit() or str(list(col)[0]).isdigit() or (str(list(col)[0]).split('-')[-1]).split(\".\")[-1].isdigit():\n","      new_data[feature]=data[feature]\n","    else:\n","      keys=list(set(list(col)))\n","      values=list(range(len(keys)))\n","      new=dict(zip(keys,values))\n","      new_data[feature]=data[feature].map(new)\n","    #new_data[features[-1]]=data[features[-1]]\n","  return new_data\n","  \n","def predict(query,tree,default = 1):\n","    #1.\n","    for key in list(query.keys()):\n","        if key in list(tree.keys()):\n","            #2.\n","            try:\n","                result = tree[key][query[key]] \n","            except:\n","                return default\n","  \n","            #3.\n","            result = tree[key][query[key]]\n","            #4.\n","            if isinstance(result,dict):\n","                return predict(query,result)\n","\n","            else:\n","                return result\n","\n","\n","def test(data,tree):\n","    #Create new query instances by simply removing the target feature column from the original dataset and \n","    #convert it to a dictionary\n","    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n","    \n","    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n","    predicted = pd.DataFrame(columns=[\"predicted\"]) \n","    \n","    #Calculate the prediction accuracy\n","    for i in range(len(data)):\n","        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n","    #print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')\n","    return predicted[\"predicted\"], data[\"class\"]\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"eEaHzjApOwC1","executionInfo":{"status":"ok","timestamp":1625769719715,"user_tz":-420,"elapsed":16,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}}},"source":["def createDataTableCsv(data):\n","    '''\n","    Takes data in form of pandas dataFrame that was extracted from .csv file. Note that the labels are manually\n","    typed in. As long as the data from .csv can return list of dataTable and list of dataLabels, the algorithm should work.\n","    '''\n","\n","    global dataTable_testing\n","    dataTable = data.values.tolist()  # Use .values to get a numpy.array and then .tolist() to get a list.\n","    shuffle(dataTable)\n","    training_size = int(len(dataTable) * 0.8)  # 90 percent of the sample after the randomization.\n","    dataTable_training = list(dataTable[0:training_size])\n","    dataTable_testing = list(dataTable[training_size:])\n","\n","    return dataTable_training,dataTable_testing"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t509fVQCXr7s"},"source":["#TEST OF DATASET"]},{"cell_type":"code","metadata":{"id":"obH165PnRx63","executionInfo":{"status":"ok","timestamp":1625769719716,"user_tz":-420,"elapsed":17,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}}},"source":["def testModel(testData):\n","  print(testData)\n","  #import data\n","  data=pd.read_csv(testData)\n","  data=convert_data_all(data,list(data.columns))\n","\n","  dataTable_training,dataTable_testing= createDataTableCsv(data)\n","  treeID3 = ID3_createTree(dataTable_training, list(data.columns))\n","  treeC45 = C45_createTree(dataTable_training, list(data.columns))\n","  treeCART = CART_createTree(dataTable_training, list(data.columns))\n","\n","  treeFWDT = FWDT_createTree(dataTable_training, list(data.columns))\n","  #test(testing_data,tree)\n","\n","  pandas_df = pd.DataFrame(dataTable_testing)\n","  pandas_df.columns=list(data.columns)\n","\n","  y_predID3,y_testID3=test(pandas_df,treeID3)\n","  y_predC45,y_testC45=test(pandas_df,treeC45)\n","  y_predCART,y_testCART=test(pandas_df,treeCART)\n","\n","  y_predFWDT,y_testFWDT=test(pandas_df,treeFWDT)\n","\n","  #Fill in the missing value if any by median method\n","  y_predID3 = y_predID3.fillna(lambda x: x.median())\n","  y_testID3=y_testID3.fillna(lambda x: x.median())\n","  y_predC45 = y_predC45.fillna(lambda x: x.median())\n","  y_testC45=y_testC45.fillna(lambda x: x.median())\n","  y_predCART = y_predCART.fillna(lambda x: x.median())\n","  y_testCART=y_testCART.fillna(lambda x: x.median())\n","\n","  y_predFWDT = y_predFWDT.fillna(lambda x: x.median())\n","  y_testFWDT=y_testFWDT.fillna(lambda x: x.median())\n","\n","  #calculate accurancy:\n","  '''\n","  print('Accurancy of CART tree: ',round((np.sum(y_testCART == y_predCART)/len(y_predCART))*100),3)\n","  print('Accurancy of ID3 tree: ',round((np.sum(y_testID3 == y_predID3)/len(y_predID3))*100),3)\n","  print('Accurancy of C45 tree: ',round((np.sum(y_testC45 == y_predC45)/len(y_predC45))*100),3)\n","  print('Accurancy of FWDT tree: ',round((np.sum(y_testFWDT == y_predFWDT)/len(y_predFWDT))*100),3)\n","  '''\n","\n","  '''\n","  classification_report\n","  '''\n","\n","  #ID3\n","  y_tID3=y_testID3.values\n","  y_pID3=y_predID3.values\n","\n","  y_tID3=y_tID3.astype(np.float)\n","  y_pID3=y_pID3.astype(np.float)\n","  print('ID3: ')\n","  print(\"Accurancy, Recall; F1-score: %s, %s, %s\"% (round((np.sum(y_testID3 == y_predID3)/len(y_predID3))*100,3),\n","        round(recall_score(y_tID3, y_pID3, average=\"macro\")*100,3),\n","        round(f1_score(y_tID3, y_pID3, average=\"macro\")*100,3)))\n","  print('\\n')\n","\n","\n","  #C45\n","  y_tC45=y_testC45.values\n","  y_pC45=y_predC45.values\n","\n","  y_tC45=y_tC45.astype(np.float)\n","  y_pC45=y_pC45.astype(np.float)\n","  print('C45: ')\n","  print(\"Accurancy, Recall; F1-score: %s, %s, %s\"% (round((np.sum(y_testC45 == y_predC45)/len(y_predC45))*100,3),\n","        round(recall_score(y_tC45, y_pC45, average=\"macro\")*100,3),\n","        round(f1_score(y_tC45, y_pC45, average=\"macro\")*100,3)))\n","  print('\\n')\n","\n","\n","  #CART\n","  y_tCART=y_testCART.values\n","  y_pCART=y_predCART.values\n","\n","  y_tCART=y_tCART.astype(np.float)\n","  y_pCART=y_pCART.astype(np.float)\n","  print('CART: ')\n","  print(\"Accurancy, Recall; F1-score: %s, %s, %s\"% (round((np.sum(y_testCART == y_predCART)/len(y_predCART))*100,3),\n","        round(recall_score(y_tCART, y_pCART, average=\"macro\")*100,3),\n","        round(f1_score(y_tCART, y_pCART, average=\"macro\")*100,3)))\n","  print('\\n')\n","  \n","  #FWDT\n","  y_tFWDT=y_testFWDT.values\n","  y_pFWDT=y_predFWDT.values\n","\n","  y_tFWDT=y_tFWDT.astype(np.float)\n","  y_pFWDT=y_pFWDT.astype(np.float)\n","  print('FWDT: ')\n","  print(\"Accurancy, Recall; F1-score: %s, %s, %s\"% (round((np.sum(y_testFWDT == y_predFWDT)/len(y_predFWDT))*100,3),\n","        round(recall_score(y_tFWDT, y_pFWDT, average=\"macro\")*100,3),\n","        round(f1_score(y_tFWDT, y_pFWDT, average=\"macro\")*100,3)))\n","  print('\\n')\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lky9z1ZcKoFZ","executionInfo":{"status":"ok","timestamp":1625769719716,"user_tz":-420,"elapsed":16,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}}},"source":["listData0=['nurseryf.csv','breast_cancerf.csv','car_evaluationf.csv',\n","      'mushroomf.csv',\n","      'spectf.csv','statlogff.csv','zoof.csv','Immunotherapy.csv','app_recordf.csv']\n","\n","listData=['nurseryf_filtered.csv','breast_cancerf_filtered.csv','car_evaluationf_filtered.csv',\n","      'mushroomf_filtered.csv',\n","      'spectf_filtered.csv','statlogff_filtered.csv','zoof_filtered.csv','Immunotherapy_filtered.csv','app_recordf_filtered.csv']\n","\n","listDataq1=['nurseryf_filtered_q1.csv','breast_cancerf_filtered_q1.csv','car_evaluationf_filtered_q1.csv',\n","      'mushroomf_filtered_q1.csv',\n","      'spectf_filtered_q1.csv','statlogff_filtered_q1.csv','zoof_filtered_q1.csv','Immunotherapy_filtered_q1.csv','app_recordf_filtered_q1.csv']"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQ0KjItdVfJn"},"source":["for i in range(len(listData0)):\n","\n"," testModel(listData0[i]),testModel(listData[i]),testModel(listDataq1[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvoOfgiifUnV","executionInfo":{"status":"ok","timestamp":1625770790465,"user_tz":-420,"elapsed":5517,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}},"outputId":"9499334a-7bac-45fc-a132-31312fea3a28"},"source":["pip install ReliefF"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Collecting ReliefF\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/f1/3d8bb05c448b3ed5e6a436166344b3aafa71848de8f5ee2595489627fc5c/ReliefF-0.1.2.tar.gz (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ReliefF) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ReliefF) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ReliefF) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ReliefF) (1.0.1)\n","Building wheels for collected packages: ReliefF\n","  Building wheel for ReliefF (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ReliefF: filename=ReliefF-0.1.2-cp37-none-any.whl size=6319 sha256=24c39d323f2a31dcd0a232c4175883ae4055ac6e58e5bffecd5bdedcc3adbd43\n","  Stored in directory: /root/.cache/pip/wheels/1c/9e/51/59ca638520794c9e0155d592b9f8c579f80cc29cbaf1de0f45\n","Successfully built ReliefF\n","Installing collected packages: ReliefF\n","Successfully installed ReliefF-0.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"lCDIEud0fM9t","executionInfo":{"status":"error","timestamp":1625770810796,"user_tz":-420,"elapsed":515,"user":{"displayName":"Van Vu LE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFZergugG2cu2m49ywMnfkobrS--fujgTln_Iayw=s64","userId":"08699602265017017536"}},"outputId":"2cf16969-8f7a-40d6-ac5e-ef122d98573e"},"source":["from ReliefF import ReliefF\n","import numpy as np\n","from sklearn import datasets\n","import pandas as pd\n","\n","#example of 2 class problem\n","data = np.array([[9,2,2],[5,1,0],[9,3,2],[8,3,1],[6,0,0]])\n","target = np.array([0,0,1,1,1])\n","\n","fs = ReliefF(n_neighbors=1, n_features_to_keep=2)\n","\n","print (fs.w_)"],"execution_count":32,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-a0a4820eb738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReliefF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'ReliefF' object has no attribute 'w_'"]}]}]}